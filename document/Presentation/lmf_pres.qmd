---
title: "Reproducible analysis pipelines using containers and data exploration using R/Shiny"
subtitle: "Máster en bioinformática y bioestadística"
author: "Luis Morís Fernández"
editor: visual
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: images/800px-Logo_blau_uoc.png
    css: styles.css
---

# The reproducibility problem

::: {.notes}
In this first section I will present the context and motivation of this work which is the problem of reproducibility in scientific research.
Reproducibility is one of the cornerstones of scientific research, nonetheless it is commonly forgotten and usually shadowed by the novelty or alleged impact of the results of a study.
:::

##

::::: {.columns}

:::: {.column width="65%"}
>["Only when certain events recur in accordance with rules or regularities, as is the case with repeatable experiments, can our observations be tested --- in principle --- by anyone. We do not take even our own observations quite seriously, or accept them as scientific observations, until we have repeated and tested them. Only by such repetitions can we convince ourselves that we are not dealing with a mere isolated 'coincidence'[...]"]{style="font-size:2rem;"} 
>
> [--- Karl R. Popper. *The Logic of Scientific Discovery* (1959)]{style="font-size:1.5rem;"} 

::::

:::: {.column width="35%"}
![](images/Karl_Popper.jpg)
::::

:::::

::: {.notes}
Already in 1959 Karl Popper stated in this quote that until our observations are reproduced we should not take them seriously or accept them as scientific observations. But even with such a clear statement done more than 60 years ago by one of the most influential personalities in the philosophy of science in the last century, reproducibility is far from achieved and is still a topic of debate in almost every field in science.
:::

## [Reproducibility Project: Cancer Biology]{style="font-size:3.5rem;"}

![](images/e-life_repro.jpg)

::: {style="margin-left:auto;margin-right:0;"}
Extracted from (Errington, Mathur, et al. 2021)
:::

::: {.notes}
Proof of this problem is the inmense work made in the Reproducibility Project in cancer biology. In this project authors attempted to reproduce the results of 50 different experiments in 23 different papers.
One of the most representative results of this study is the graph shown in the slide, the x asis represents the original effect size of the study while the y axis represent the effect size in the replication. The graph on the right is a zoomed version of the the studies with effect size lower than five. The ideal result in this graph would be to have most of the points along the diagonal of the graph, meaning that the effect sizes in the original study and the replication study were the same. Unfortunately, most of the points are scattered along the value 0 in the y axis. Meaning that it was not possible to replicate the results of the original studies.
IN fact authors found a suprisingly low reproducibility rate of about 40% for those studies with positive effects.

But, reproduciblity can be adressed at more levels than reproducing results in independent experiments. Being one of its most basic levels reproducing the results of an study useing the same data and analysis procedure.
:::

## «Authors provide all the necessary data and the computer codes to run the analysis again, re-creating the results»
 (Barba, 2018)
 
::: {.notes}
In this work we will address reproducibility as defined by Barba in 2018.
:::

## The open policy data example

![](images/Hardwicke_1.png)

::: {style="margin-left:auto;margin-right:0;"}
Extracted from (Hardwicke et al. 2018)
:::

. . .


**Most papers were less than 2 months old!**

. . .


What about older papers?

::: {.notes}
In 2018 Tom Hardwicke and other examined the effect of an open data policy in the journal Cognition. Within the scope of this work the authors evaluated how often they could reproduce the results of the original papers using the description found in the manuscript and the data provided by the authors. Suprisingly, in 69% of the assesments at least one error was found, albeit most of them were resolved once the authors were contacted. (PAUSE) Although this may seem like a satisfactory result, in fact authors were still very familiar with the analysis and results as papers were only, at most, two months old. One then can only ask themselves what would be the result of the study if papers were older. Would authors remember the exact nature of the analysis and be able to reproduce their results?
:::

## Reproducibility pioneers

> An article should be considered an «advertising» and that the full research work is «the complete software development environment and the complete set of instructions that generated the figures»
>
> [--- Morís Fernández paraphrasing Buckeit and Donoho paraphrasing Claerbout & Karrenback]{style="font-size:1.5rem;"} 

::: {.notes}
Attempts to solve this problem has been under discussion for more than 30 years, already the pioneers Claerbout & Karrenback highlighted the need of reproducing the results and not only to write a manuscript, but to include the the 'complete software development environment and the complete set of instructions that generated the figures'. At the moment their solution involved using CD-ROMS, magnetical tapes and the (and still in use) makefile tool. In this work it is illustrated how to use current tools, workflow manager, specifically targets and light virtualization tools, specifically Docker to ameliorate this problem of reproducibility in the field of bioinformatics.
:::

## Workflow managers

![](images/wratten_1.png)

::: {style="margin-left:auto;margin-right:0;"}
Extracted from (Wratten et al. 2021)
:::

::: {.notes}
Workflow managers can be found nowadays in many different flavors as described by Wratten and others, from very generic tools applicable in almost any domain such as Apache Airflow to those focused on scientific analysis, such as Scipipe or targets or even those specific to the bioninformatics field such as Galaxy and KNIME. The different workflow managers also differ in its features, for example from those built and used usually around a Graphical user interface but with maybe a limited expresiveness ,to those code-based less focused on usability and more in achieving a powerful and expressive tool.
In the following slides we will describe the rational followed to select targets workflow manager for this work.
:::

## Why targets?

::::: {.columns}

:::: {.column width="30%"}
![](images/R_logo.svg.png)
::::

:::: {.column width="70%"}
::: {style="margin-left:auto; margin-right:0;float: right;"}
![](images/bioconductor_logo_rgb.png)
:::
::::

:::::

::: {.notes}
The first valuable feature is that this workflow manager is written in R. R (R Core Team 2022) is one of the most common languages used in bioinformatics and data analysis in general (Giorgi, Ceraolo, and Mercatelli 2022). And Bioconductor, also R-based is one of the main repositories for the analysis of biological data. Therefore using targets allows using all the tools available in Bioconductor directly in our pipeline. Also, given the prevalence of R in bioinformatics, and other data science fields, many users will already be familiar with the language itself. For users already familiar with R, targets itself brings a small learning overhead.
:::

## Why targets?

- Automatic tracking of dependencies

![](images/DAG.png)

::: {.notes}
targets package by design also automatically keeps track of the dependencies in the different pipeline steps, therefore, when the pipeline is modified and rerun many time consuming calculations are skipped if the modifications do not affect them. This is particularly beneficial in the case of bioinformatics given that many of the data analysis steps include long computations. This dependency tracking also makes the pipeline easily reproducible as it guarantees that no invalid information is used and the results will be consistent when running the pipeline.
:::

## Why targets?

- Handles paralellization with minor configuration

- Easily managed in control version systems

- General purpose and decoupled from bioinformatic packages

::: {.notes}
Regarding scalability, targets is prepared to run single-threaded, locally in parallel or in high-performance computing environments with just simple changes in the declaration of the pipeline.
As all the content in targets pipelines consist of text files this makes the pipelines defined using targets ideal for versioning control systems (e.g.: git).
Another advantage is that targets is a general purpose solution, offering a very concrete functionality and is highly decoupled, from a software perspective, from the packages offering the analysis functionality, this means that changes in those packages can be incorporated in our pipelines without waiting for any update in targets itself.
:::

## Issues with targets?

- Code based, not ideal for all users

- Focused in R

. . .

- No control of versions or the environment

. . .

![](images/Docker-Logo-768x432.jpg)

::: {.notes}
Nonetheless, targets does have some shortcomings. The fact that it is a code-based solution, is not ideal for users with no, or few, programming experience. In those cases other solutions such as Galaxy or KNIME maybe more attractive and offer a more smooth learning curve.
Although, R has interfaces with Python and can call any other language using system calls, other packages have more straightforward approaches to include tools and scripts programmed in other languages. For example, Nextflow allows calling any scripting language in the same pipeline with almost no overhead.
(PAUSE)
Finally, one substantial difference with other more complex workflow managers is that  targets does not keep track or controls the version of software or the environment where the pipeline is executed. This can be easily overcome by using complimentary R packages that control the versions in the R library such as renv (PAUSE), or using Docker as is the case of this work.
:::

## [The dependency hell and rotting software problem]{style="font-size:3rem;"}

::::: {.columns}

:::: {.column width="50%"}
![](images/1 mHrDuetdLskvNHYucD9u3g.png)

::: {style="margin-left:auto; margin-right:0;float: right; font-size:1.5rem"}
From: https://xkcd.com/1987/
:::

::::

:::: {.column width="50%"}
::: {style="font-size:1.75rem"}
>"Software rot [...] is either a slow deterioration of software quality over time [...] that will eventually lead to software becoming faulty, unusable, or in need of upgrade. This is not a physical phenomenon: the software does not actually decay, but rather suffers from a lack of being responsive and updated **with respect to the changing environment in which it resides**."
>
> --- https://en.wikipedia.org/wiki/Software_rot
:::
::::

:::::

::: {.notes}
Controlling the executing environment of a software has been a challenging task for many years as humurously described by this XKCD cartoon. Software developers and system administrators coined the term «Dependency Hell» to describe the common situation in which a  «Software A» depended on a given version of a library while «Software B» depended on a different version of the same library creating a complicated situation for installing «Software A» and «Software B» in the same environment. This situation was particularly troubling when installing «Software B» automatically updated the required library making «Software A» in the best case unusable or in the worst case silently providing erroneous outputs. Software rotting is a related problem in which a software becomes faulty and unusable with time, as the enviroment in which it resides slowly changes. These problems were inherited and expanded as soon as researchers started using computational methods and writing analysis scripts.

Controlling the environment, from the OS to the software versions and libraries, are fundamental to be able to recreate the results of a given study. It could be easy to recreate the results days just after the initial analysis was done, recreating it months or even years after the analysis happened may be more complicated, for example due to software rotting. R versions, versions of R packages, and Bioconductor packages change very often. Just in 2022 four different versions of R and two Bioconductor versions were released as well as five versions of the Ubuntu operating system.
:::

## Docker

![](images/docker-img-1.png)

::: {style="margin-left:auto; margin-right:0;float: right; font-size:1.5rem"}
From: https://accesto.com/blog/when-to-use-and-when-not-to-use-docker/
:::

::: {.notes}
A Docker container is a piece of software that can pack the full environment, in our case OS, system libraries, R, R libraries, Bioconductor libraries, etc.  Allowing us to recreate the full set of results in any machine and in any time in the future, even if R, or any of the libraries used, would no longer be available or become obsolete.
:::

## Reducing the user-analyst loop

![](images/403-4036373_shiny-shiny-r-logo-png-transparent-png.png)

::: {.notes}
Finally, in this work an interactive tool using R/Shiny was created to reduce the loop between the data analysts and the final consumers of the results. It is commonly the case that the same person is not in charge of analyzing data and making decisions on the results of the analysis. Sometimes this creates a loop between the data analyst and the decision maker until the correct set of results required by the decision maker is achieved and a decision can be made. A way of improving this situation is empowering the decision maker to modify some of the settings in the final analysis, without the intervention of the data-analyst, through interactive applications. 
:::

<!-- ## Methodology & materials -->

<!-- - maUEB (Ferrer Almirall and Camacho Cano 2022) -->

<!-- - targets (Landau 2021) -->

<!-- - Docker (Merkel 2014) -->

<!-- - R/Shiny (Chang et al. 2022) -->




# Results

- Targets microarray Analysis pipeline

- Containerizing in a Docker

- An interactive application for data exploration using R/Shiny

::: {.notes}
Once the context of the work is established in this following section we will present the results for the three outputs of this work.
:::

## [Targets microarray analysis pipeline: Steps]{style="font-size:3rem;"}

::::: {.columns style="font-size:1.25rem"}

:::: {.column width="45%"}
1. **Data loading**
2. **Quality control**
    1.  Raw expression data
        1. Create a boxplot figure
        2. Perform a Principal Component Analysis (PCA)
        3. Evaluate sample distance
        4. Create an arrayQualityMetrics package reported
    2. Data normalization
    3. Annotate normalized data
    4. Same quality control steps fornormalized data
    5. Artifactual or outlier removal
    6. Plot standard deviations of the probesets
    7. Remove, duplicated or low-variance probesets
    8. Annotate filtered data

::::

:::: {.column width="45%"}
3. **Differential Expression Analysis (DEA)**
    1. Create a design matrix for the analysis
    2. Fit the linear model to the data
    3. Calculate the relevant contrasts for the model
    4. Gather results of the linear model comparisons:
        1. tables with the results of the contrasts
        2. Create a summary table for each contrasts
        3. Create a volcano plot of the contrasts
        4. Create a Venn diagram of the contrasts 
        5. Create a heatmap figure of the results
4. **Gene Set Enrichment and Over-representation analysis**
6. **Rmarkdown report**
::::
:::::

- `maUEB` (Ferrer Almirall and Camacho Cano 2022)

::: {.notes}
The first output of this work was a microarray analysis pipeline programmed using the maUEB package with the following sections (READ STEPS) each composed by several steps.
As the main focus of this work was the implementation inside targets and not the pipeline itself, the exact contents of the analysis pipeline will not be discussed and we will focus on the advantages of using targets and of the particular implementation done in this work.
:::

## [Targets microarray analysis pipeline: Advantages]{style="font-size:3rem;"}

- Step behavior is defined by a list of parameters

- Each section has an specific list of parameters

- All parameters are packed in a single section of the pipeline

- User can focus exclusively on the parameter lists instead of modyfing the pipeline itself

::: {.notes}
This worked tried to favour the usability therefore an special effort was done to minimize the number of changes needed to modify the pipeline, and to concentrate all changes in a single section of the pipeline, preventing the user from having to navigate the whole script when making a change. For this a list of parameters was defined for each the sections of the pipeline that defined the behavior and outputs for the steps in each section. Additionally if a given step shared the same set of parameters, parameters could be easily reused between steps, minimizing the possiblity of mistakenly modifying the parameters in one step and not in another.
:::

## [Targets microarray analysis pipeline: Advantages]{style="font-size:3rem;"}

- Code is based on small multiples

```{.r}
  tar_target(
    name = qc_raw_boxplot_file,
  command = do.call(
    what = maUEB::qc_boxplot %>% as_list_of_files("outputDir"),
      args = purrr::list_modify(
        !!qcl_par$raw$boxplot,
      data = eset_raw
      )
      ),
    format = "file"
  )
```

- Avoids forcing the user to navigate code to change the pipeline behavior

::: {.notes}
When implementing the pipeline we used a small multiple approach meaning all the targets were defined following the same pattern.
A maUEB function was invoked, the predefined list of parameters and the input from previous steps was passed always the same way.
This helps including or removing steps from the pipeline in a consistent way and, as mentioned before, makes all changes in the pipeline local to the
parameter lists and avoids changes all along the pipeline.
:::

## [Targets microarray analysis pipeline: A use case]{style="font-size:3rem;"}

- ["A user finds an outlier sample that wants to exclude from the analysis"]{style="font-size:2rem;"}

```{.r}
# Before
outlier_samples <- "CMP.2"
# After
outlier_samples <- c("CMP.2", "PD1.1")
```

::::: {.columns}

:::: {.column width="50%"}
![](images/DAG.png)

::::

:::: {.column width="50%"}
![](images/DAG_out.png)

::::

:::::

```{.r}
tar_make()
```

::: {.notes}
To make the example more concrete, a hypothetical modification in the pipeline is presented. Lets imagine that after the reviewing the quality control data an anomalous sample is detected. The only change needed is adding a new sample in the parameters for the quality control section, as shown in the code block. After the change all outputs affected by the modification are automatically outdated (dark green outputs become light blue). When the whole pipelines is rerun using targets::tar_make() only the outdated targets will be recalculated, while all other targets will be skipped. A workflow manager not provided with this dependency tracking would force the user to decide which outputs were affected by this change and rerun them. A much error-prone approach.
:::

## Containerizing in a Docker

- Based on Rocker (Boettiger and Eddelbuettel 2017)

- Convenience scripts are provided

```{.bash}
docker run --name target_trial -v $(pwd)/:/workspace -w /workspace
--rm uoc:final Rscript -e "targets_::tar_make()"
```

::: {.notes}
A docker image able to run the whole pipeline was created. This image was based on an image provided by the rocker project that provides Docker images with R installations in different flavors (Boettiger and Eddelbuettel 2017).
A convenience script is provided to run the whole targets pipeline inside the Docker image.
:::

## An interactive application for data exploration using R/Shiny{style="font-size:1.5rem;"}

![](images/shiny.png)

- Created using Shiny modules

::: {.notes}

Demo app

1. un/adjusted p-value
  
2. log Fold Change limits
  
3. CSV download

4. Volcano plot
  
5. Info box
  
6. Upset chart
:::

# Final remarks

::: {.notes}
Now a quick summary of the achievements of the work are presented
:::

## A targets containerized microarray pipeline

- Users can concentrate on a smaller portion of the script for their changes

- Target declaration complexity was minimized by using small multiples

- Easily reproducible and automatic tracking of dependencies

::: {.notes}
The targets containerized microarray pipeline allows users to concentrate their changes on a single place.

The target declaration was simplified by declaring them in a consistent way using small multiples

Tracking dependency improves the reproducibility of the pipeline and minimizes errors
:::

## Containerizing in a Docker

- Easily generalized to other pipelines

- Easy to archive and use in the future

- Helpful solving the reproducibility problem

::: {.notes}
The docker image could be easily adapted to other pipelines

It is easy to archive, for example in docker hub, or internal repositories to reuse
and to improve the reproducibility of the results
:::

## An interactive application for data exploration using R/Shiny

- Simple but effective in reducing the data-analyst vs data-decision-makers loops

- Easy to modify and recompose thanks to modular approach

::: {.notes}
Although simple, the app it may be effective in reducing the data analyst decision maker loop.
As it is programmed using Shiny modules the app components can be reused in a an improved app once more functionality required by users is detected.
:::

# Future lines of work

::: {.notes}
Finally we present several improvements to the current work.
:::

## A targets containerized microarray pipeline

- A more extensive manual on the pipeline could be written

- Additional versions of this pipeline could be written for other analyses

- Another strategies for creating the parameters list could be explored, separate files, configuration files

## Containerizing in a Docker

- Data analysis scripts and data itself could be included in the Docker image for improved reproducibility

- Docker image could be uploaded to a Docker image repository for direct use by researchers 

## An interactive application for data exploration using R/Shiny

- Run refinement sessions with users to prioritize new functionalities

- Add more interactive graphs to the Shiny applications

- Include the interactive graphs inside an Rmarkdown report instead of a Shiny application

- Explore alternatives to Shiny

  - Javascript

::: {.notes}
  - Download only genes that are significant in several comparisons
  
  - Provide more links to external databases with information about the genes
  
   javascript widgets that could be seen without the need of a Shiny runtime environment and an R installation and stored and shared as standalone files
   
:::

# The end

**Thanks for your time and attention**
