---
title: "Reproducible analysis pipelines using containers and data exploration using R/Shiny"
author: "Luis Morís Fernández"
editor: visual
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: images/800px-Logo_blau_uoc.png
    css: styles.css
---

# The reproducibility problem

::: {.notes}
Reproducibility is one of the cornerstones of scientific research, nonetheless it is commonly forgotten and usually shadowed by the novelty or alleged impact of the results of a study.
:::

::::: {.columns}

:::: {.column width="65%"}
>["Only when certain events recur in accordance with rules or regularities, as is the case with repeatable experiments, can our observations be tested --- in principle --- by anyone. [...] Only by such repetitions can we convince ourselves that we are not dealing with a mere isolated 'coincidence'[...]"]{style="font-size:2rem;"} 
>
> [--- Karl R. Popper. *The Logic of Scientific Discovery* (1959)]{style="font-size:1.5rem;"} 

::::

:::: {.column width="35%"}
![](images/Karl_Popper.jpg)
::::

:::::

::: {.notes}
Already in 1959 Karl Popper stated in this quote that only by repetition could we convince ourselves that we are not dealing with mere isolated 'coincidence'. But even with such a clear statement such as the one in the current slide more than 60 years ago by one of the most influential personalities in the philosophy of science. Reproducibility is far from achieved and is still a topic of debate in almost every field in science.
:::

## [Reproducibility Project: Cancer Biology]{style="font-size:3.5rem;"}

![](images/e-life_repro.jpg)

::: {style="margin-left:auto;margin-right:0;"}
Extracted from (Errington, Mathur, et al. 2021)
:::

::: {.notes}
Proof of this problem is the inmense work made in the Reproducibility Project in cancer biology. One of the most representative results of this study is the graph shown in the slide, the x asis represents the original effect size of the study while the y axis represent the effect size in the replication. The graph on the left is a zoomed version of the the studies with effect size lower than five. The ideal result would be to have most of the points along the diagonal of the graph, meaning that the results in the original study and the replication study were the same. Unfortunately, most of the points are scattered along the value 0 in the y axis. Meaning that it was not possible to replicate the results of the original studies.
In this project authors attempted to reproduce the results of 50 different experiments in 23 different papers. With a success rate of 40% for those studies with positive effects. Nonetheless, reproduciblity goes beyond just reproducing results in independent experiments with one of its most basic level being reproducing the results of an study or experiment given its data and analysis procedure.
:::

## «Authors provide all the necessary data and the computer codes to run the analysis again, re-creating the results»
 (Barba, 2018)

## The open policy data example

![](images/Hardwicke_1.png)

::: {style="margin-left:auto;margin-right:0;"}
Extracted from (Hardwicke et al. 2018)
:::

. . .


**Most papers were less than 2 months old!**

. . .


What about older papers?

::: {.notes}
In 2018 Tom Hardwicke and other examined the effect of an open data policy in the journal Cognition. Within the scope of this work the authors evaluated how often they could reproduce the results of the original papers using the description found in the manuscript and the data provided by the authors. Astonishingly, in 69% of the assesments at least one error was found, albeit most of them were resolved once the authors were contacted. (PAUSE) Although this may seem like a satisfactory result, in fact authors were still very familiar with the analysis and results as papers were only, at most, two months old. One then can only ask himself what would be the result of the study if papers were old. Would authors remember the exact nature of the analysis and be able to reproduce their results?
:::

## Reproducibility pioneers

> An article should be considered an «advertising» and that the full research work is «the complete software development environment and the complete set of instructions that generated the figures»
>
> [--- Morís Fernández paraphrasing Buckeit and Donoho paraphrasing Claerbout & Karrenback]{style="font-size:1.5rem;"} 

::: {.notes}
Attempts to solve this problem has been under discussion for more than 30 years, already the pioneers Claerbout & Karrenback highlighted the need of reproducing the results and not only to write a manuscript, but to include the the 'complete software development environment and the complete set of instructions that generated the figures'. At the moment their solution involved using CD-ROMS, magnetical tapes and the (and still in use) makefile tool. In this work it is illustrated how to use current tools, workflow manager, specifically targets and light virtualization tools, specifically Docker to ameliorate this problem of reproducibility in the field of bioinformatics.
:::

## Workflow managers

![](images/wratten_1.png)

::: {style="margin-left:auto;margin-right:0;"}
Extracted from (Wratten et al. 2021)
:::

::: {.notes}
Workflow managers can be found nowadays in many different flavors, from very generic tools applicable in almost any domain such as Apache Airflow to those focused on scientific analysis, such as Scipipe or targets and those specific to the bioninformatics domain such as Galaxy and KNIME. The different workflow managers are also not similar in its features from those built and used usually around a Graphical user interface but with maybe a limited expresiveness to those less focused on usability and more in achieving a powerful and expressive tool.
In the following slides we will describe the rational followed to select targets for this work.
:::

## Why targets?

::::: {.columns}

:::: {.column width="30%"}
![](images/R_logo.svg.png)
::::

:::: {.column width="70%"}
::: {style="margin-left:auto; margin-right:0;float: right;"}
![](images/bioconductor_logo_rgb.png)
:::
::::

:::::

::: {.notes}
The first valuable feature is that this workflow manager is written in R. R (R Core Team 2022) is one of the most common languages used in bioinformatics and data analysis in general (Giorgi, Ceraolo, and Mercatelli 2022). Bioconductor, one of the main repositories for the analysis of biological data, contains tools programmed in R. Therefore using targets allows using all the tools available in Bioconductor directly in our pipeline1. Also, given the prevalence of R in bioinformatics, and other data science fields, many users will already be familiar with the language itself. For users already familiar with R, targets itself brings a small learning overhead, as most of the programming usually correspond to the task themselves and not so much to the  pipeline preparation.
:::

## Why targets?

- Automatic tracking of dependencies

![](images/DAG.png)

::: {.notes}
targets package by design also automatically keeps track of the dependencies in the different pipeline steps, therefore, when the pipeline is modified and rerun many time consuming calculations are skipped if the modifications do not affect them. This is particularly beneficial in the case of bioinformatics given that many of the data analysis steps include long computations. This dependency tracking also makes the pipeline easily reproducible as it guarantees that no invalid information is used and the results will be consistent when running the pipeline. It is also easy to track which information is used as an input in each of the tasks.
:::

## Why targets?

- Handles paralellization with minor configuration

- Easily managed in control version systems

- General purpose and decoupled from bioinformatic packages

::: {.notes}
Regarding scalability, targets is prepared to run single-threaded, locally in parallel or in high-performance computing environments with just simple changes in the declaration of the pipeline.
As all the content in targets pipelines consist of text files (with the exceptions of inputs and outputs) this makes the pipelines defined using targets ideal for versioning control systems (e.g.: git).
targets is a general purpose solution, offering a very concrete functionality and is highly decoupled, from a software perspective, from the packages offering the analysis functionality, this means that changes in those packages can be incorporated without waiting for any update in targets.
:::

## Issues with targets?

- Code based, not ideal for all users

- Focused in R
. . .

- No control of versions or the environment

. . .
![](images/Docker-Logo-768x432.jpg)

::: {.notes}
Nonetheless, targets does have some shortcomings. The fact that it is a code-based solution, is not ideal for users with no, or few, programming experience. In those cases other solutions such as Galaxy or KNIME maybe more attractive and offer a more smooth learning curve, in fact both of them offer interfaces to the R language so they have access to R packages functionality at the cost of making them available in the web interface a bit more cumbersome. Albeit, R has interfaces with Python and, as mentioned before, can run Python code  reticulate and can call any other language using system calls, other packages have more straightforward approaches to solve this problem. For example, Nextflow allows calling any scripting language in the same pipeline with almost no overhead.
Finally, one substantial difference with other more complex workflow managers is that  targets does not keep track or controls the version of software or the environment where the pipeline is executed. This can be easily overcome by using complimentary R packages that control the versions in the R library such as renv (Ushey, 2022), or, as covered in a following section of this work, running the pipeline inside a Docker container.
:::

## [The dependency hell and rotting software problem]{style="font-size:3rem;"}

::::: {.columns}

:::: {.column width="50%"}
![](images/1 mHrDuetdLskvNHYucD9u3g.png)

::: {style="margin-left:auto; margin-right:0;float: right; font-size:1.5rem"}
From: https://xkcd.com/1987/
:::

::::

:::: {.column width="50%"}
::: {style="font-size:1.75rem"}
>"Software rot [...] is either a slow deterioration of software quality over time [...] that will eventually lead to software becoming faulty, unusable, or in need of upgrade. This is not a physical phenomenon: the software does not actually decay, but rather suffers from a lack of being responsive and updated **with respect to the changing environment in which it resides**."
>
> --- https://en.wikipedia.org/wiki/Software_rot
:::
::::

:::::

::: {.notes}
Controlling the executing environment of a software has been a challenging task for many years. Software developers and system administrators coined the term «Dependency Hell» (2023) to describe the common situation in which a  «Software A» depended on a given version «Library Z» while «Software B» depended on a different version of «Library Z» creating a complicated situation for installing «Software A» and «Software B» in the same environment. This situation was particularly troubling when installing «Software B» automatically updated «Library Z» making «Software A» in the best case unusable or in the worst case silently providing erroneous outputs. A similar kind of problem appeared when a  piece of software was discontinued for the new version of a given operating system forcing the user to find an alternative software that  provided the same or similar functionality. These problems were inherited and expanded as soon as researchers started using computational methods and writing analysis scripts.
Controlling the environment, from the OS to the software versions and libraries, are fundamental to be able to recreate the results of a given study. Albeit, arguably, it could be easier to recreate the results days just after the initial analysis was done, recreating it months or even years after the analysis happened may be more complicated. R versions, versions of R packages, and Bioconductor packages change very often. Just in 2022 four different versions of R (Index of /src/base/R-4, 2022) and two Bioconductor versions were released (Bioconductor Releases, 2022) as well as five versions of the Ubuntu operating system. Therefore, controlling the exact versions used for creating them in first place is fundamental. On top of that, it is important that other researchers can recreate this results in different machines so they can build a new research on top, or maybe run the same analysis on their own set of data. Containers, and particularly Docker containers, are a common solution for these problems nowadays.
:::

## Docker

![](images/docker-img-1.png)

::: {style="margin-left:auto; margin-right:0;float: right; font-size:1.5rem"}
From: https://accesto.com/blog/when-to-use-and-when-not-to-use-docker/
:::

::: {.notes}
A Docker container is a piece of software that packs up code and dependencies and allows to an application to be run in different computing environments or at different times. Because it packs both code and dependencies it recreates a full environment, in our case: OS, system libraries, R, R libraries, Bioconductor libraries, etc. Allowing us to recreate the full set of results in any machine and in any time in the future, even if R, or any of the libraries used, would no longer be available.
:::

## Reducing the user-analyst loop

![](images/403-4036373_shiny-shiny-r-logo-png-transparent-png.png)

::: {.notes}
Finally, in this work an interactive tool was created to reduce the loop between the data analysts and the final consumers of the results. It is commonly the case that the same person is not in charge of analyzing data and making decisions on the results of this analysis. Sometimes this creates a loop between the data analyst and the decision maker until the correct set of results required by the decision maker is achieved and a decision can be made. A way of improving this situation is empowering the decision maker to modify some of the setting in the final analysis, without the intervention of the data-analyst, through interactive applications. 
:::

## Methodology & materials

- maUEB (Ferrer Almirall and Camacho Cano 2022)

- targets (Landau 2021)

- Docker (Merkel 2014)

- R/Shiny (Chang et al. 2022)

# Results

## [Targets microarray analysis pipeline: Steps]{style="font-size:3rem;"}

1. Data Loading

2. Quality Control

3. Differential Expression Analysis

4. Gene Set enrichment Analysis

## [Targets microarray analysis pipeline: Advantages]{style="font-size:3rem;"}

- Step behavior is defined by a list of parameters

- Each step has an specific list of parameters

- All parameters are packed in a single section of the pipeline

- User can focus exclusively on the parameter lists instead of modyfing the pipeline itself

## [Targets microarray analysis pipeline: Advantages]{style="font-size:3rem;"}

- Code is based on small multiples

```{.r}
  tar_target(
    name = qc_raw_boxplot_file,
  command = do.call(
    what = maUEB::qc_boxplot %>% as_list_of_files("outputDir"),
      args = purrr::list_modify(
        !!qcl_par$raw$boxplot,
      data = eset_raw
      )
      ),
    format = "file"
  )
```

- Avoids forcing the user to navigate code to change the pipeline behavior

## [Targets microarray analysis pipeline: A use case]{style="font-size:3rem;"}

- ["A user finds an outlier sample that wants to exclude from the analysis"]{style="font-size:2rem;"}

```{.r}
# Before
outlier_samples <- "CMP.2"
# After
outlier_samples <- c("CMP.2", "PD1.1")
```

::::: {.columns}

:::: {.column width="50%"}
![](images/DAG.png)

::::

:::: {.column width="50%"}
![](images/DAG_out.png)

::::

:::::

```{.r}
tar_make()
```

::: {.notes}
Here an example of a modification in the pipeline is presented. Lets imagine that after the reviewing the quality control data of the raw and normalized data  an additional anomalous sample is detected. The only change needed is adding a new sample in the  qcl_par$outlier_samples entry, as shown in Code 11. Notice  in Figure 6 that, after the change in  Code 11, all targets depending on eset_raw_f are automatically outdated and when the whole pipelines is rerun using targets::tar_make(), or just part of it if a particular target is specified in the call, only the outdated targets will be recalculated, while all other targets will be skipped.
:::

## Containerizing in a Docker

- Based on Rocker (Boettiger and Eddelbuettel 2017)

- Libraries were frozen in a given date

## An interactive application for data exploration using R/Shiny{style="font-size:1.5rem;"}

![](images/shiny.png)


::: {.notes}

1. un/adjusted p-value
  
2. log Fold Change limots
  
3. CSV download

4. Volcano plot
  
5. Info box
  
6. Upset chart
:::

- Created using Shiny modules
    
# Discussion

## A targets containerized microarray pipeline

- Users can concentrate on a smaller portion of the script for their changes

- Target declaration complexity was minimized by using small multiples

- Easily reproducible and automatic tracking of dependencies

## Containerizing in a Docker

- Easily generalized to other pipelines

- Easy to archive and use in the future

- Helpful solving the reproducibility problem

## An interactive application for data exploration using R/Shiny

- Simple but effective in reducing the data-analyst vs data-decision-makers loops

- Reusable can composable

# Conclusions

## List of achieved objectives

1. Describe the reproducibility problem in bioinformatics

2. Explore containers and workflow tools as a mean to improve the reproducibility of bioinformatics pipelines

3. Explore interactive tools as a mean to improve the decision making loop in clinical settings

4. Create a microarray analysis pipeline using containers that produce a report

# Future lines of work

## A targets containerized microarray pipeline

- Modifications to the maUEB package could be done to allow implementing an interactive report

- A more extensive manual on the pipeline could be written

- Additional versions of this pipeline could be written for other analyses

- Another strategies for creating the parameters list could be explored

## Containerizing in a Docker

- Data analysis could be included in the Docker

- Docker image could be uploaded to a Docker image repository for direct use by researchers 

## An interactive application for data exploration using R/Shiny

- Run refinement sessions with users to prioritize new functionalities, for example:

  - Download only genes that are significant in several comparisons
  
  - Provide more links to external databases with information about the genes
  
- Add more interactive graphs to the Shiny applications

- Include the interactive graphs inside an Rmarkdown report instead of a Shiny application

- Explore alternatives to Shiny to add interactivity, for example, including javascript widgets that could be seen without the need of a Shiny runtime environment and an R installation and stored and shared as standalone files

# The end

**Thanks for your time and attention**
